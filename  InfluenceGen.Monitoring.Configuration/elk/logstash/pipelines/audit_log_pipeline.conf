# Purpose: Logstash pipeline for structured audit logs.
# (REQ-12-001, REQ-12-002, REQ-12-006, REQ-ATEL-001, REQ-ATEL-002, REQ-ATEL-004, REQ-ATEL-005, REQ-ATEL-006)

input {
  beats {
    port => 5047 # Separate port for audit logs from Filebeat
    # Add SSL/TLS if needed
  }
  # Or other input suitable for the audit log source (e.g., reading from a specific file)
}

filter {
  # Filter for logs tagged as audit
  if [fields][app_name] == "audit" or [agent][name] =~ "filebeat.*audit" {

    # Assume audit logs are already in JSON format
    json {
      source => "message"
      target => "audit_event" # Parse JSON into a nested field
      remove_field => ["message"]
      tag_on_failure => ["_jsonparsefailure_audit"]
    }

    # Check if JSON parsing was successful
    if "_jsonparsefailure_audit" not in [tags] {

      # Mutate and add common fields
      mutate {
        add_field => { "app_name" => "audit" }
        # Ensure host.name is captured from beats
        add_field => { "host.name" => "%{[host][name]}" }
      }

      # Date filter for the audit log timestamp field (REQ-ATEL-006)
      # Assuming the JSON contains a field named 'timestamp' in ISO8601 format
      date {
        match => [ "[audit_event][timestamp]", "ISO8601" ]
        target => "@timestamp"
        timezone => "UTC" # Ensure UTC
        tag_on_failure => ["_dateparsefailure_audit"]
      }

      # Rename/copy fields to match requirements (REQ-ATEL-006)
      # Adjust paths based on your actual audit log JSON structure
      mutate {
        copy => { "[audit_event][eventType]" => "eventType" }
        copy => { "[audit_event][actorUserId]" => "actorUserId" }
        copy => { "[audit_event][targetEntity]" => "targetEntity" }
        copy => { "[audit_event][targetId]" => "targetId" }
        copy => { "[audit_event][action]" => "action" }
        copy => { "[audit_event][details]" => "details" }
        copy => { "[audit_event][ipAddress]" => "ipAddress" }
        # Add outcome field if present
        copy => { "[audit_event][outcome]" => "outcome" }
      }

      # Example: Geocode IP Address if needed (requires geoip database)
      # geoip {
      #   source => "ipAddress"
      # }

    } else {
       # Handle JSON parse failures - perhaps move to a separate index or log
    }

  } else {
    # Drop logs not tagged as audit
    drop {}
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"] # Elasticsearch service name and port
    index => "influencegen-audit-logs-%{+YYYY.MM.dd}" # Dynamic index name for audit logs
    user => "${ES_USER}" # Injected
    password => "${ES_PASSWORD}" # Injected
    # manage_template => false # Assume templates are managed externally
    # ssl => true
    # ssl_certificate_verification => true
    # cacert => "/etc/logstash/certs/ca.crt"
  }
  # stdout { codec => rubydebug } # For debugging
}